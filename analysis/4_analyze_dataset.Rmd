---
title: "Analyze dataset"
date: "`r Sys.Date()`"
bibliography: references.bib
biblio-style: apalike
---

## About

### Description

This section is by far the busiest of the four that precede the article. In this section bar charts and maps are created for each of the groups of terms, and chi-squared tests are conducted on each to determine the nature of each term's distribution. These three features serve as the foundation for all prose analysis about the data.

### Usage

After loading in the necessary packages, code should be run for each updated CSV creating a bar chart, a contingency table, and a map, as well as code conducting a chi-square test and further analysis on the results of this test. The chunks necessary for these actions are labeled below, and the only thing that needs to be changed out is the name of the updated CSV.

## Setup

```{r setup}
# Script-specific options or packages
library(janitor)
library(patchwork)
library(broom)
library(effectsize)
```

## Run

<!-- Steps involved in analyzing the data -->

### Second Person Plurals

```{r plurals-contingency}
updated_plurals %>% 
  tabyl(search_term, census_region) %>% # cross-tabulate
  adorn_totals(c("row", "col")) %>% # provide row and column totals
  adorn_percentages("col") %>% # add percentages to the columns
  adorn_pct_formatting(rounding = "half up", digits = 0) %>% # round the digits
  adorn_ns() %>% # add observation number
  adorn_title("combined") %>% # add a header title
  kable(booktabs = TRUE, # pretty table
        caption = "Contingency table for `search_term` and `census_region`.") # caption
```

```{r plurals-barchart, echo=TRUE}
p1 <- 
  updated_plurals %>% # dataset
  ggplot(aes(x = census_region, fill = search_term)) + # mappings
  geom_bar() + # geometry
  labs(y = "Count", x = "Census Region") # labels

p2 <- 
  updated_plurals %>% # dataset
  ggplot(aes(x = census_region, fill = search_term)) + # mappings
  geom_bar(position = "fill") + # geometry, with fill for proportion plot
  labs(y = "Proportion", x = "Census Region", fill = "Modality") # labels

p1 <- p1 + theme(legend.position = "none") # remove legend from left plot

p1 + p2 + plot_annotation("Distribution of Second Person Plurals Across the United States")
```

```{r plurals-chi-test, warning=FALSE}
ror_mod_table <- 
  xtabs(formula = ~ census_region + search_term, # formula 
        data = updated_plurals) # dataset

c2 <- chisq.test(ror_mod_table) # apply the chi-squared test to `ror_mod_table`

c2 # # preview the test results
#> 
#>  Pearson's Chi-squared test with Yates' continuity correction
#> 
#> data:  ror_mod_table
#> X-squared = 101, df = 1, p-value <2e-16

c2$p.value < .05 # confirm p-value below .05
#> [1] TRUE
```

```{r plurals-chi-deets}
c2 %>% # statistical result
  augment() # view detailed statistical test information
```

```{r plurals-chi-ci}
effects <- effectsize(c2)  # evaluate effect size and generate a confidence interval

effects  # preview effect size and confidence interval
#> Cramer's V |       95% CI
#> -------------------------
#> 0.18       | [0.14, 0.21]

interpret_r(effects$Cramers_v)  # interpret the effect size
#> [1] "small"
#> (Rules: funder2019)
```

```{r second-person-plurals-plot, warning=FALSE}
states_map <- map_data("state")  # from ggplot2

p <- ggplot() + geom_polygon(data = states_map, aes(x = long, y = lat, group = group),
    fill = "grey", color = "black") + labs(title = "Tweets in the USA", subtitle = "Second Person Plurals")

p + geom_point(data = plurals, aes(x = lng, y = lat, group = 1, color = search_term),
    alpha = 1/2, size = 1.5)
```

### Outdoor Sales

```{r sales-contingency}
updated_sales %>% 
  tabyl(search_term, census_region) %>% # cross-tabulate
  adorn_totals(c("row", "col")) %>% # provide row and column totals
  adorn_percentages("col") %>% # add percentages to the columns
  adorn_pct_formatting(rounding = "half up", digits = 0) %>% # round the digits
  adorn_ns() %>% # add observation number
  adorn_title("combined") %>% # add a header title
  kable(booktabs = TRUE, # pretty table
        caption = "Contingency table for `search_term` and `census_region`.") # caption
```

```{r sales-barchart}
p1 <- 
  updated_sales %>% # dataset
  ggplot(aes(x = census_region, fill = search_term)) + # mappings
  geom_bar() + # geometry
  labs(y = "Count", x = "Census Region") # labels

p2 <- 
  updated_sales %>% # dataset
  ggplot(aes(x = census_region, fill = search_term)) + # mappings
  geom_bar(position = "fill") + # geometry, with fill for proportion plot
  labs(y = "Proportion", x = "Census Region", fill = "Modality") # labels

p1 <- p1 + theme(legend.position = "none") # remove legend from left plot

p1 + p2 + plot_annotation("Distribution of Terms for Outdoor Sales Across the US")
```

```{r sales-chi-test, warning=FALSE}
ror_mod_table <- 
  xtabs(formula = ~ census_region + search_term, # formula 
        data = updated_sales) # dataset

c2 <- chisq.test(ror_mod_table) # apply the chi-squared test to `ror_mod_table`

c2 # # preview the test results
#> 
#>  Pearson's Chi-squared test with Yates' continuity correction
#> 
#> data:  ror_mod_table
#> X-squared = 101, df = 1, p-value <2e-16

c2$p.value < .05 # confirm p-value below .05
#> [1] TRUE
```

```{r sales-chi-deets}
c2 %>% # statistical result
  augment() # view detailed statistical test information
```

```{r sale-chi-ci}
effects <- effectsize(c2)  # evaluate effect size and generate a confidence interval

effects  # preview effect size and confidence interval
#> Cramer's V |       95% CI
#> -------------------------
#> 0.18       | [0.14, 0.21]

interpret_r(effects$Cramers_v)  # interpret the effect size
#> [1] "small"
#> (Rules: funder2019)
```

```{r garage-sale-plot, warning=FALSE}
states_map <- map_data("state")  # from ggplot2

p <- ggplot() + geom_polygon(data = states_map, aes(x = long, y = lat, group = group),
    fill = "grey", color = "black") + labs(title = "Tweets in the USA", subtitle = "Outdoor Sales")

p + geom_point(data = sale, aes(x = lng, y = lat, group = 1, color = search_term),
    alpha = 1/2, size = 1.5)
```

### Gym Shoes

```{r shoes-contingency}
updated_shoes %>% 
  tabyl(search_term, census_region) %>% # cross-tabulate
  adorn_totals(c("row", "col")) %>% # provide row and column totals
  adorn_percentages("col") %>% # add percentages to the columns
  adorn_pct_formatting(rounding = "half up", digits = 0) %>% # round the digits
  adorn_ns() %>% # add observation number
  adorn_title("combined") %>% # add a header title
  kable(booktabs = TRUE, # pretty table
        caption = "Contingency table for `search_term` and `census_region`.") # caption
```

```{r shoes-barchart}
p1 <- 
  updated_shoes %>% # dataset
  ggplot(aes(x = census_region, fill = search_term)) + # mappings
  geom_bar() + # geometry
  labs(y = "Count", x = "Census Region") # labels

p2 <- 
  updated_shoes %>% # dataset
  ggplot(aes(x = census_region, fill = search_term)) + # mappings
  geom_bar(position = "fill") + # geometry, with fill for proportion plot
  labs(y = "Proportion", x = "Census Region", fill = "Modality") # labels

p1 <- p1 + theme(legend.position = "none") # remove legend from left plot

p1 + p2 + plot_annotation("Distribution of Terms for Gym Shoes Across the United States")
```

```{r shoes-chi-test}
ror_mod_table <- 
  xtabs(formula = ~ census_region + search_term, # formula 
        data = updated_shoes) # dataset

c2 <- chisq.test(ror_mod_table) # apply the chi-squared test to `ror_mod_table`

c2 # # preview the test results
#> 
#>  Pearson's Chi-squared test with Yates' continuity correction
#> 
#> data:  ror_mod_table
#> X-squared = 101, df = 1, p-value <2e-16

c2$p.value < .05 # confirm p-value below .05
#> [1] TRUE
```

```{r shoes-chi-deets}
c2 %>% # statistical result
  augment() # view detailed statistical test information
```

```{r shoes-chi-ci}
effects <- effectsize(c2)  # evaluate effect size and generate a confidence interval

effects  # preview effect size and confidence interval
#> Cramer's V |       95% CI
#> -------------------------
#> 0.18       | [0.14, 0.21]

interpret_r(effects$Cramers_v)  # interpret the effect size
#> [1] "small"
#> (Rules: funder2019)
```

```{r shoes-plot, warning=FALSE}
states_map <- map_data("state")  # from ggplot2

p <- ggplot() + geom_polygon(data = states_map, aes(x = long, y = lat, group = group),
    fill = "grey", color = "black") + labs(title = "Tweets in the USA", subtitle = "Shoes")

p + geom_point(data = shoes, aes(x = lng, y = lat, group = 1, color = search_term),
    alpha = 1/2, size = 1.5)
```

### Soft Drinks

```{r tonix-contingency}
updated_tonix %>% 
  tabyl(search_term, census_region) %>% # cross-tabulate
  adorn_totals(c("row", "col")) %>% # provide row and column totals
  adorn_percentages("col") %>% # add percentages to the columns
  adorn_pct_formatting(rounding = "half up", digits = 0) %>% # round the digits
  adorn_ns() %>% # add observation number
  adorn_title("combined") %>% # add a header title
  kable(booktabs = TRUE, # pretty table
        caption = "Contingency table for `search_term` and `census_region`.") # caption
```

```{r tonix-barchart}
p1 <- 
  updated_tonix %>% # dataset
  ggplot(aes(x = census_region, fill = search_term)) + # mappings
  geom_bar() + # geometry
  labs(y = "Count", x = "Census Region") # labels

p2 <- 
  updated_tonix %>% # dataset
  ggplot(aes(x = census_region, fill = search_term)) + # mappings
  geom_bar(position = "fill") + # geometry, with fill for proportion plot
  labs(y = "Proportion", x = "Census Region", fill = "Modality") # labels

p1 <- p1 + theme(legend.position = "none") # remove legend from left plot

p1 + p2 + plot_annotation("Distribution of Terms for Soft Drinks Across the US")
```

```{r tonix-chi-test}
ror_mod_table <- 
  xtabs(formula = ~ census_region + search_term, # formula 
        data = updated_tonix) # dataset

c2 <- chisq.test(ror_mod_table) # apply the chi-squared test to `ror_mod_table`

c2 # # preview the test results
#> 
#>  Pearson's Chi-squared test with Yates' continuity correction
#> 
#> data:  ror_mod_table
#> X-squared = 101, df = 1, p-value <2e-16

c2$p.value < .05 # confirm p-value below .05
#> [1] TRUE
```

```{r tonix-chi-deets}
c2 %>% # statistical result
  augment() # view detailed statistical test information
```

```{r tonix-chi-ci}
effects <- effectsize(c2)  # evaluate effect size and generate a confidence interval

effects  # preview effect size and confidence interval
#> Cramer's V |       95% CI
#> -------------------------
#> 0.18       | [0.14, 0.21]

interpret_r(effects$Cramers_v)  # interpret the effect size
#> [1] "small"
#> (Rules: funder2019)
```

```{r tonix-plot}
states_map <- map_data("state")  # from ggplot2

p <- ggplot() + geom_polygon(data = states_map, aes(x = long, y = lat, group = group),
    fill = "grey", color = "black") + labs(title = "Tweets in the USA", subtitle = "Soft Drinks")

p + geom_point(data = tonix, aes(x = lng, y = lat, group = 1, color = search_term),
    alpha = 1/2, size = 1.5)
```

### Major Roads

```{r roads-contingency}
updated_roads %>% 
  tabyl(search_term, census_region) %>% # cross-tabulate
  adorn_totals(c("row", "col")) %>% # provide row and column totals
  adorn_percentages("col") %>% # add percentages to the columns
  adorn_pct_formatting(rounding = "half up", digits = 0) %>% # round the digits
  adorn_ns() %>% # add observation number
  adorn_title("combined") %>% # add a header title
  kable(booktabs = TRUE, # pretty table
        caption = "Contingency table for `search_term` and `census_region`.") # caption
```

```{r roads-barchart}
p1 <- 
  updated_roads %>% # dataset
  ggplot(aes(x = census_region, fill = search_term)) + # mappings
  geom_bar() + # geometry
  labs(y = "Count", x = "Census Region") # labels

p2 <- 
  updated_roads %>% # dataset
  ggplot(aes(x = census_region, fill = search_term)) + # mappings
  geom_bar(position = "fill") + # geometry, with fill for proportion plot
  labs(y = "Proportion", x = "Census Region", fill = "Modality") # labels

p1 <- p1 + theme(legend.position = "none") # remove legend from left plot

p1 + p2 + plot_annotation("Distribution of Terms for Major Roads Across the US")
```

```{r roads-chi-test}
ror_mod_table <- 
  xtabs(formula = ~ census_region + search_term, # formula 
        data = updated_roads) # dataset

c2 <- chisq.test(ror_mod_table) # apply the chi-squared test to `ror_mod_table`

c2 # # preview the test results
#> 
#>  Pearson's Chi-squared test with Yates' continuity correction
#> 
#> data:  ror_mod_table
#> X-squared = 101, df = 1, p-value <2e-16

c2$p.value < .05 # confirm p-value below .05
#> [1] TRUE
```

```{r roads-chi-deets}
c2 %>% # statistical result
  augment() # view detailed statistical test information
```

```{r roads-chi-ci}
effects <- effectsize(c2)  # evaluate effect size and generate a confidence interval

effects  # preview effect size and confidence interval
#> Cramer's V |       95% CI
#> -------------------------
#> 0.18       | [0.14, 0.21]

interpret_r(effects$Cramers_v)  # interpret the effect size
#> [1] "small"
#> (Rules: funder2019)
```

```{r roads-plot, warning=FALSE}
states_map <- map_data("state")  # from ggplot2

p <- ggplot() + geom_polygon(data = states_map, aes(x = long, y = lat, group = group),
    fill = "grey", color = "black") + labs(title = "Tweets in the USA", subtitle = "Major Roads")

p + geom_point(data = roads, aes(x = lng, y = lat, group = 1, color = search_term),
    alpha = 1/2, size = 1.5)
```

## Finalize

### Log

This code should produce bar charts, contingency tables, maps, and information about each chi-squaured test conducted.

## References

  Vaux, Bert. “Harvard Dialect Survey.” Harvard Dialect Survey, 2003, http://dialect.redlog.net.